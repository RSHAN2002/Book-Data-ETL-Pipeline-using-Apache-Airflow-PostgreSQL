# ğŸ“š Book Data Pipeline

## ğŸš€ Overview
This project is an **ETL (Extract, Transform, Load) data pipeline** that fetches book data from multiple sources (**NYTimes API, OpenLibrary API, Google Books API**), processes it, and loads it into a **PostgreSQL** database using **Apache Airflow**.

## ğŸ—ï¸ Project Structure
```
book_data_pipeline/
â”‚â”€â”€ dags/                         # Airflow DAGs (ETL Workflow)
â”‚   â”œâ”€â”€ book_pipeline.py          # Main DAG defining ETL tasks
â”‚   â”œâ”€â”€ tasks/
â”‚       â”œâ”€â”€ fetch_data.py         # Fetch book data from APIs
â”‚       â”œâ”€â”€ transform_data.py     # Process and clean data
â”‚       â”œâ”€â”€ load_data.py          # Load data into PostgreSQL
â”‚       â”œâ”€â”€ data_quality.py       # Validate loaded data
â”‚â”€â”€ db/                           # Database setup files
â”‚   â”œâ”€â”€ init.sql                  # PostgreSQL schema
â”‚â”€â”€ Dockerfile                    # Airflow Docker setup
â”‚â”€â”€ docker-compose.yml            # Multi-container setup
â”‚â”€â”€ config/                       # Configuration files
â”‚   â”œâ”€â”€ .env                      # Environment variables
â”‚â”€â”€ requirements.txt              # Dependencies
â”‚â”€â”€ README.md                     # Documentation
```

---

## ğŸ”§ Setup and Installation

### 1ï¸âƒ£ **Clone the Repository**
```bash
git clone https://github.com/your-repo/book_data_pipeline.git
cd book_data_pipeline
```

### 2ï¸âƒ£ **Set Up Environment Variables**
Create a `.env` file in the `config/` folder:
```
NYTIMES_API_KEY=your_nytimes_api_key
GOOGLE_BOOKS_API_KEY=your_google_books_api_key
DB_CONN=postgresql://postgres:1234@postgres:5432/books_db
```

### 3ï¸âƒ£ **Run the Pipeline with Docker**

```bash
docker-compose up --build
```
This starts **PostgreSQL, Airflow Webserver, Scheduler, and Worker** containers.

```bash
docker-compose run --rm airflow-webserver airflow db init
```
This initilize the database if needed

```bash
docker-compose run --rm airflow-webserver airflow users create  --username admin --password admin123  --firstname Admin --lastname User --role Admin --email admin@example.com
```
This defines ***role and admin*** for airflow webserver

### 4ï¸âƒ£ **Access Airflow UI**
ğŸ“Œ **URL:** [http://localhost:8080](http://localhost:8080)  
**Login:** Username: `admin` | Password: `admin123`

### 5ï¸âƒ£ **Trigger the DAG**
- Navigate to `book_data_pipeline`
- Click **Trigger DAG** (â–¶ï¸)

---

## ğŸ› ï¸ How It Works
1ï¸âƒ£ **Fetch Data** (`fetch_data.py`) - Collects book data from NYTimes, OpenLibrary, and Google Books.  
2ï¸âƒ£ **Transform Data** (`transform_data.py`) - Cleans and enriches the data.  
3ï¸âƒ£ **Load Data** (`load_data.py`) - Inserts data into PostgreSQL.  
4ï¸âƒ£ **Validate Data** (`data_quality.py`) - Ensures data integrity.

---

## ğŸ“Š Database Schema (`init.sql`)
```sql
CREATE TABLE public.books (
    id SERIAL PRIMARY KEY,
    title TEXT NOT NULL,
    author TEXT NOT NULL,
    publisher TEXT DEFAULT 'Unknown Publisher',
    published_date TEXT,
    isbn TEXT UNIQUE NOT NULL,
    genres TEXT[],
    description TEXT,
    rank INT CHECK (rank >= 0),
    list_name TEXT,
    weeks_on_list INT CHECK (weeks_on_list >= 0),
    page_count INT CHECK (page_count >= 0),
    language TEXT DEFAULT 'Unknown',
    average_rating FLOAT CHECK (average_rating >= 0 AND average_rating <= 5),
    ratings_count INT CHECK (ratings_count >= 0),
    cover_image_url TEXT,
    buy_links TEXT[],
    data_source TEXT DEFAULT 'NYTimes, OpenLibrary, GoogleBooks',
    ingested_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

---

## ğŸ› ï¸ Debugging & Logs
ğŸ“Œ **Check Logs for Errors:**
```bash
docker logs -f book_data_pipeline-airflow-webserver-1
```

ğŸ“Œ **Check PostgreSQL Data:**
```bash
docker exec -it book_data_pipeline-postgres-1 psql -U postgres -d books_db -c "SELECT * FROM books LIMIT 5;"
```

---

## ğŸ Stopping & Restarting
To stop all services:
```bash
docker-compose down
```
To restart with fresh data:
```bash
docker-compose down
docker volume rm book_data_pipeline_postgres_data
docker-compose up --build
```

---

## ğŸ“ Future Improvements
- Add more book data sources (e.g., Goodreads API)
- Implement automated email notifications for pipeline failures
- Enhance data quality checks (e.g., duplicate detection)

---

### ğŸš€ **Your ETL Pipeline is Ready!** ğŸ‰
Let me know if you need further refinements! âœ…

